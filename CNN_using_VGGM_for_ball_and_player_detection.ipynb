{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivans14/Deep_Learning_Proj/blob/main/CNN_using_VGGM_for_ball_and_player_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e59f02",
      "metadata": {
        "id": "50e59f02"
      },
      "source": [
        "# Ball and players detection for SoccerNet\n",
        "### Using CNN and the pretained model VGGM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d98dbff6",
      "metadata": {
        "id": "d98dbff6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tkinter\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('TkAgg')\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import LocalResponseNorm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import os\n",
        "import cv2\n",
        "from files.engine import train_one_epoch, evaluate, hi\n",
        "\n",
        "import files.utils as utils\n",
        "import files.transforms as T\n",
        "from tkinter import *\n",
        "# for image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "abf99ee7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "IPN27JHX3Ra5",
      "metadata": {
        "id": "IPN27JHX3Ra5"
      },
      "outputs": [],
      "source": [
        "# Import dataset as: train, validation and test splits\n",
        "\n",
        "# data = pd.read_csv('proj_test/proj_det/det.txt')\n",
        "# images= ['proj_test/proj_img1/{}.jpg'.format(str(i).zfill(6)) for i in range(1,750)]\n",
        "# images[1]\n",
        "# int(images[1][-10:-4])\n",
        "\n",
        "# Data split\n",
        "directory_train = 'proj_test/train'\n",
        "\n",
        "# Load Train data\n",
        "train_labels = pd.read_csv(directory_train + \"/proj_det/det.txt\", sep=',')\n",
        "train_images = directory_train + \"/proj_img1/{}.jpg\"\n",
        "train_images = [train_images.format(str(i).zfill(6)) for i in train_labels.index.values]\n",
        "train_labels = [train_labels.columns.values.tolist()] + train_labels.values.tolist() # Convert DF to list\n",
        "\n",
        "#print(train_labels[0:5][0:7])\n",
        "\n",
        "# Load Test data\n",
        "directory_test = 'proj_test/test'\n",
        "\n",
        "test_labels = pd.read_csv(directory_test + '/proj_det/det.txt', sep=',', index_col=0)\n",
        "test_images = directory_test + \"/proj_img1/{}.jpg\"\n",
        "test_images = [test_images.format(str(i).zfill(6)) for i in test_labels.frame.values]\n",
        "test_labels = [test_labels.columns.values.tolist()] + test_labels.values.tolist() # Convert DF to list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "68c45f52",
      "metadata": {
        "id": "68c45f52"
      },
      "outputs": [],
      "source": [
        "# Create image dataset class\n",
        "class img_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, height, width, dir_img, labels_list, images, transforms=None, mode='train') -> None:\n",
        "        self.transforms = transforms\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.dir_img = dir_img\n",
        "        self.labels_list = labels_list\n",
        "        self.images=images\n",
        "\n",
        "    def __getitem__(self,id):\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        img_name = self.images[id]\n",
        "        \n",
        "\n",
        "        # reading the images and converting them to correct size and color    \n",
        "        img = cv2.imread(img_name)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "        # diving by 255\n",
        "        img_res /= 255.0\n",
        "        \n",
        "        for line in range(1, len(self.labels_list)):\n",
        "            #parsed = [float(x) for x in line.split(',')]\n",
        "            if int(self.labels_list[line][0]) == int(self.images[id][-10:-4]):\n",
        "                x = self.labels_list[line][2]\n",
        "                y = self.labels_list[line][3]\n",
        "                width = self.labels_list[line][4]\n",
        "                height = self.labels_list[line][5]\n",
        "                x_max = x + width\n",
        "                y_max = y + height\n",
        "                boxes.append([x,y,x_max,y_max])\n",
        "                labels.append(self.labels_list[line][7])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        print(boxes)\n",
        "        print(boxes.shape)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        target[\"area\"] = area\n",
        "        image_id = torch.tensor([id])\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image = img_res,\n",
        "                                        bboxes = target['boxes'],\n",
        "                                        labels = labels)\n",
        "            img_res = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "            return img_res, target\n",
        "\n",
        "        return img_res,boxes\n",
        "         \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f59d9b4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create transformations fo the dataset to increase the data.\n",
        "# Send train=True for training transforms and False for val/test transforms\n",
        "def get_transform(train):\n",
        "  if train:\n",
        "    return A.Compose(\n",
        "      [\n",
        "        A.HorizontalFlip(0.5),\n",
        "        # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "        ToTensorV2(p=1.0) \n",
        "      ],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )\n",
        "  else:\n",
        "    return A.Compose(\n",
        "      [ToTensorV2(p=1.0)],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "45b0601e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 929.,  488.,  974.,  624.],\n",
            "        [ 401.,  344.,  434.,  450.],\n",
            "        [   4.,  328.,   42.,  431.],\n",
            "        [ 253.,  537.,  308.,  695.],\n",
            "        [ 757.,  529.,  809.,  671.],\n",
            "        [ 302.,  865.,  406., 1079.],\n",
            "        [  24.,  666.,  154.,  835.],\n",
            "        [ 114.,  333.,  148.,  426.],\n",
            "        [ 735.,  244.,  770.,  321.],\n",
            "        [1537.,  525., 1577.,  663.],\n",
            "        [1255.,  610., 1277.,  634.],\n",
            "        [1229.,  607., 1281.,  791.]])\n",
            "torch.Size([12, 4])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 1080, 1920])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds = img_dataset(1080,1920,directory_train + \"/proj_img1/\",train_labels,train_images, transforms=get_transform(train=True))\n",
        "test_ds = img_dataset(1080,1920,directory_test + \"/proj_img1/\",test_labels,test_images, mode='test', transforms=get_transform(train=False))\n",
        "ran_img, boxes = train_ds[20]\n",
        "ran_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ef3eefd3",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Invalid shape (3, 1080, 1920) for image data",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-18-b50cf434d28b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# plt.show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mplot_img_bbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mran_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-18-b50cf434d28b>\u001b[0m in \u001b[0;36mplot_img_bbox\u001b[1;34m(img, boxes)\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_size_inches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[1;31m# img_plot = mpimg.imread(img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m   \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m   \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5523\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    709\u001b[0m         if not (self._A.ndim == 2\n\u001b[0;32m    710\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m--> 711\u001b[1;33m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[0;32m    712\u001b[0m                             .format(self._A.shape))\n\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: Invalid shape (3, 1080, 1920) for image data"
          ]
        }
      ],
      "source": [
        "#Visualize the data\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def plot_img_bbox(img, boxes):\n",
        "  # plot the image and bboxes\n",
        "  # Bounding boxes are defined as follows: x-min y-min width height\n",
        "  fig, a = plt.subplots(1,1)\n",
        "  fig.set_size_inches(5,5)\n",
        "  # img_plot = mpimg.imread(img)\n",
        "  a.imshow(img)\n",
        "  count = 0\n",
        "  for box in range(0,boxes.shape[0]):\n",
        "    print(box)\n",
        "    print(box[1], \" and \", box[2])\n",
        "    x, y, width, height  = float(box[0]), float(box[1]), float(box[2]) - float(box[0]), float(box[3]) - float(box[1])\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 2,\n",
        "      edgecolor = 'r',\n",
        "      facecolor = 'none'\n",
        "    )\n",
        "    # Draw the bounding box on top of the image\n",
        "    a.add_patch(rect)\n",
        "    a.annotate(str(count), xy=(x,y))\n",
        "    count += 1\n",
        "  plt.show()\n",
        "\n",
        "# print(boxes)\n",
        "# img = mpimg.imread(ran_img)\n",
        "# imgplot = plt.imshow(img)\n",
        "# plt.show()\n",
        "\n",
        "plot_img_bbox(ran_img, boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "48c01c0f",
      "metadata": {
        "id": "48c01c0f"
      },
      "outputs": [],
      "source": [
        "# # CNN model\n",
        "# out_features = 2 # Ball and players\n",
        "# image_shape = (400, 400, 3)\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#     def __init__(self,out_classes, image_size):\n",
        "#         super().__init__()\n",
        "#         self.out_classes = out_classes\n",
        "#         H = image_size[0]\n",
        "#         W = image_size[1]\n",
        "#         C = image_size[2]\n",
        "\n",
        "#         conv1 = {\n",
        "#             \"in\": 1,\n",
        "#             \"out\": 16,\n",
        "#             \"kernel\": 5,\n",
        "#             \"stride\": 2,\n",
        "#             \"padding\": 0\n",
        "#         }\n",
        "#         conv2 = {\n",
        "#             \"in\":conv1[\"out\"],\n",
        "#             \"out\": 28,\n",
        "#             \"kernel\": 5,\n",
        "#             \"stride\": 2,\n",
        "#             \"padding\": 0\n",
        "#         }\n",
        "#         pool = {\n",
        "#             \"kernel\": 2,\n",
        "#             \"stride\": 2,\n",
        "#             \"padding\": 0\n",
        "#         }\n",
        "\n",
        "#         conv_param = [conv1,pool,conv2,pool]\n",
        "\n",
        "#         for i in range (0, len(conv_param)):\n",
        "#             H = np.floor((H + 2*conv_param[i][\"padding\"] - conv_param[i][\"kernel\"])/conv_param[i][\"stride\"] + 1)\n",
        "#             W = np.floor((W + 2*conv_param[i][\"padding\"] - conv_param[i][\"kernel\"])/conv_param[i][\"stride\"] + 1)\n",
        "\n",
        "\n",
        "#         activation_fn = nn.ReLU(0.1)\n",
        "#         # Defining a Sequential pipeline for the entire CNN\n",
        "#         self.net = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels = conv_param[0][\"in\"],\n",
        "#                      out_channels = conv_param[0][\"out\"],\n",
        "#                      kernel_size = conv_param[0][\"kernel\"],\n",
        "#                      stride= conv_param[0][\"stride\"],\n",
        "#                      padding= conv_param[0][\"padding\"]),\n",
        "#             activation_fn,\n",
        "#             LocalResponseNorm(2),   # Check: https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac\n",
        "#             nn.MaxPool2d(kernel_size = conv_param[1][\"kernel\"],\n",
        "#                         stride =conv_param[1][\"stride\"]),\n",
        "            \n",
        "#             nn.Conv2d(in_channels = conv_param[2][\"in\"],\n",
        "#                      out_channels = conv_param[2][\"out\"],\n",
        "#                      kernel_size = conv_param[2][\"kernel\"],\n",
        "#                      stride= conv_param[2][\"stride\"],\n",
        "#                      padding= conv_param[2][\"padding\"]),\n",
        "#             nn.MaxPool2d(kernel_size = conv_param[3][\"kernel\"],\n",
        "#                         stride =conv_param[3][\"stride\"]),\n",
        "            \n",
        "#             #nn.Flatten(), # Flattens a contiguous range of dims into a tensor.\n",
        "#             # FFNN\n",
        "#             activation_fn,\n",
        "#             nn.Linear(int(conv_param[-2][\"out\"] * H * W), out_features),\n",
        "#             # nn.Dropout(0.5),\n",
        "#             # nn.Sigmoid(),\n",
        "#             # nn.Linear(,self.out_classes),\n",
        "#             # nn.Dropout(0.25),\n",
        "#             # nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#       return nn.Softmax(self.net(x))\n",
        "\n",
        "# NN = Model(out_features, image_shape)\n",
        "# device = device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')  # use cuda if possible\n",
        "# NN.to(device)\n",
        "# print(NN)\n",
        "\n",
        "# # Definition of optimizer and loss functions\n",
        "# optimizer = optim.Adam(NN.parameters(),lr=0.008)\n",
        "# loss_fn = nn.CrossEntropyLoss() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de4f36b",
      "metadata": {},
      "source": [
        "## DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9090d729",
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset = img_dataset(1080,1920,'proj_test/proj_img1/',transforms=get_transform(train=True))\n",
        "# dataset_test = img_dataset(1080,1920,'proj_test/proj_img1/',transforms=get_transform(train=True))\n",
        "\n",
        "# # split the dataset in train and test set\n",
        "# torch.manual_seed(1)\n",
        "# indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "# # train test split\n",
        "# test_split = 0.2\n",
        "# tsize = int(len(dataset)*test_split)\n",
        "# dataset = torch.utils.data.Subset(dataset, indices[:10])\n",
        "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "  train_ds,\n",
        "  batch_size=1,\n",
        "  shuffle=True,\n",
        "  num_workers=0,\n",
        "  collate_fn=utils.collate_fn,\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "  test_ds,\n",
        "  batch_size=10,\n",
        "  shuffle=False,\n",
        "  num_workers=0,\n",
        "  collate_fn=utils.collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e449442",
      "metadata": {},
      "source": [
        "PRETRAINED MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "954c4986",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Faster RESNET 50 pre-trained model :)\n",
        "def get_object_detection_model(num_classes):\n",
        "  # load a model pre-trained pre-trained on COCO\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ad0eab",
      "metadata": {},
      "source": [
        "TRAINING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6b636299",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = 2 #CLASS ZERO FOR BACKGROUND, 1 FOR PLAYERS AND 2 FOR BALL \n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "  optimizer,\n",
        "  step_size=3,\n",
        "  gamma=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2d072d89",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c1c93609",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proj_test/proj_img1/000313.jpg\n",
            "Epoch: [0]  [ 0/10]  eta: 0:03:30  lr: 0.000560  loss: 0.6870 (0.6870)  loss_classifier: 0.5921 (0.5921)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.0730 (0.0730)  loss_rpn_box_reg: 0.0218 (0.0218)  time: 21.0822  data: 0.0996  max mem: 0\n",
            "proj_test/proj_img1/000162.jpg\n",
            "proj_test/proj_img1/000278.jpg\n",
            "proj_test/proj_img1/000391.jpg\n",
            "proj_test/proj_img1/000090.jpg\n",
            "proj_test/proj_img1/000181.jpg\n",
            "proj_test/proj_img1/000125.jpg\n",
            "proj_test/proj_img1/000562.jpg\n",
            "proj_test/proj_img1/000156.jpg\n",
            "proj_test/proj_img1/000587.jpg\n"
          ]
        }
      ],
      "source": [
        "# training for 5 epochs\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # training for one epoch\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "# Save model (i.e. its weights)\n",
        "models_dir = \"trained_models/\"\n",
        "torch.save(model.state_dict(), models_dir + 'CNN_weights_{}.pth'.format(len([entry for entry in os.listdir(models_dir) if os.path.isfile(os.path.join(models_dir,entry))])))\n",
        "print(\"Finished training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ismoSfa33bBb",
      "metadata": {
        "id": "ismoSfa33bBb"
      },
      "outputs": [],
      "source": [
        "# Define training function\n",
        "# HOLA \n",
        "# adios\n",
        "\n",
        "def accuracy(target, pred):\n",
        "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
        "\n",
        "# Set number of epochs according to computational power, time and results.\n",
        "batch_size = 64\n",
        "num_epochs = 4\n",
        "validation_every_steps = 500\n",
        "\n",
        "step = 0\n",
        "NN.train()\n",
        "\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "running_loss = 0\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_accuracies_batches = []\n",
        "    train_loss_batches = []\n",
        "    \n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        # Forward pass, compute gradients, perform one training step.\n",
        "        # Your code here!\n",
        "        output = NN(inputs) # Pass the inouts through the NN\n",
        "        loss = loss_fn(output,targets) # Compute and Save loss\n",
        "        optimizer.zero_grad() #Clean up gradients\n",
        "        loss.backward() # Compute gradients based on the loss from the current batch\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Increment step counter\n",
        "        step += 1\n",
        "        \n",
        "        # Compute accuracy.\n",
        "        predictions = output.max(1)[1]\n",
        "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
        "        \n",
        "        if step % validation_every_steps == 0:\n",
        "            \n",
        "            validation_loss = 0\n",
        "            # Append average training accuracy to list.\n",
        "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
        "            \n",
        "            train_accuracies_batches = []\n",
        "        \n",
        "            # Compute accuracies on validation set.\n",
        "            valid_accuracies_batches = []\n",
        "            with torch.no_grad():\n",
        "                NN.eval()\n",
        "                for inputs, targets in test_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    output = NN(inputs)\n",
        "                    loss = loss_fn(output, targets)\n",
        "\n",
        "                    predictions = output.max(1)[1]\n",
        "\n",
        "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
        "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
        "\n",
        "                NN.train()\n",
        "                \n",
        "            # Append average validation accuracy to list.\n",
        "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
        "     \n",
        "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
        "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
        "\n",
        "# Save model (i.e. its weights)\n",
        "models_dir = \"trained_models/\"\n",
        "torch.save(model.state_dict(), models_dir + 'CNN_weights.pth')\n",
        "print(\"Finished training.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
