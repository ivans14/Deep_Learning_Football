{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivans14/Deep_Learning_Proj/blob/main/CNN_using_VGGM_for_ball_and_player_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e59f02",
      "metadata": {
        "id": "50e59f02"
      },
      "source": [
        "# Ball and players detection for SoccerNet\n",
        "### Using CNN and the pretained model VGGM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d98dbff6",
      "metadata": {
        "id": "d98dbff6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('TkAgg')\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import LocalResponseNorm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import os\n",
        "import cv2\n",
        "from files.engine import train_one_epoch, evaluate, hi\n",
        "\n",
        "import files.utils as utils\n",
        "import files.transforms as T\n",
        "from tkinter import *\n",
        "# for image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "abf99ee7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "IPN27JHX3Ra5",
      "metadata": {
        "id": "IPN27JHX3Ra5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import dataset as: train, validation and test splits\n",
        "\n",
        "data = pd.read_csv('proj_test/proj_det/det.txt')\n",
        "images= ['proj_test/proj_img1/{}.jpg'.format(str(i).zfill(6)) for i in range(1,750)]\n",
        "images[1]\n",
        "int(images[1][-10:-4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "68c45f52",
      "metadata": {
        "id": "68c45f52"
      },
      "outputs": [],
      "source": [
        "# Create image dataset class\n",
        "class img_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, height, width, dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.dir = dir\n",
        "        self.images=images\n",
        "\n",
        "    def __getitem__(self,id):\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        img_name = self.images[id]\n",
        "        print(img_name)\n",
        "\n",
        "        # reading the images and converting them to correct size and color    \n",
        "        img = cv2.imread(img_name)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "        # diving by 255\n",
        "        img_res /= 255.0\n",
        "        with open ('proj_test/proj_det/det.txt') as f:\n",
        "            next(f)\n",
        "            for line in f:\n",
        "                line.strip(\"/n\")\n",
        "                parsed = [float(x) for x in line.split(',')]\n",
        "                if int(parsed[0]) == int(self.images[id][-10:-4]):\n",
        "                    x = parsed[2]\n",
        "                    y = parsed[3]\n",
        "                    width = parsed[4]\n",
        "                    height = parsed[5]\n",
        "                    x_max = x + width\n",
        "                    y_max = y + height\n",
        "                    boxes.append([x,y,x_max,y_max])\n",
        "                    labels.append(int(parsed[-1]))\n",
        "        \n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        target[\"area\"] = area\n",
        "        image_id = torch.tensor([id])\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image = img_res,\n",
        "                                        bboxes = target['boxes'],\n",
        "                                        labels = labels)\n",
        "            img_res = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "            return img_res, target\n",
        "\n",
        "        # img_res = torch.Tensor(img_res)\n",
        "\n",
        "        return img_res,boxes\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "45b0601e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proj_test/proj_img1/000021.jpg\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1080, 1920, 3)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img_ds = img_dataset(1080,1920,'proj_test/proj_img1/')\n",
        "ran_img, boxes = img_ds[20]\n",
        "ran_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ef3eefd3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proj_test/proj_img1/000011.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mirun\\AppData\\Local\\Temp\\ipykernel_17320\\3181187.py:28: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "#Visualize the data\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def plot_img_bbox(img, boxes):\n",
        "  # plot the image and bboxes\n",
        "  # Bounding boxes are defined as follows: x-min y-min width height\n",
        "  fig, a = plt.subplots(1,1)\n",
        "  fig.set_size_inches(5,5)\n",
        "  # img_plot = mpimg.imread(img)\n",
        "  a.imshow(img)\n",
        "  count = 0\n",
        "  for box in boxes:\n",
        "\n",
        "    x, y, width, height  = box[0], box[1], box[2] - box[0], box[3] - box[1]\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 2,\n",
        "      edgecolor = 'r',\n",
        "      facecolor = 'none'\n",
        "    )\n",
        "    # Draw the bounding box on top of the image\n",
        "    a.add_patch(rect)\n",
        "    a.annotate(str(count), xy=(x,y))\n",
        "    count += 1\n",
        "  plt.show()\n",
        "img_ds = img_dataset(1080,1920,'proj_test/proj_img1/')\n",
        "ran_img, boxes = img_ds[10]\n",
        "# print(boxes)\n",
        "# img = mpimg.imread(ran_img)\n",
        "# imgplot = plt.imshow(img)\n",
        "# plt.show()\n",
        "\n",
        "plot_img_bbox(ran_img, boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "48c01c0f",
      "metadata": {
        "id": "48c01c0f"
      },
      "outputs": [],
      "source": [
        "# # CNN model\n",
        "# out_features = 2 # Ball and players\n",
        "# image_shape = (400, 400, 3)\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#     def __init__(self,out_classes, image_size):\n",
        "#         super().__init__()\n",
        "#         self.out_classes = out_classes\n",
        "#         H = image_size[0]\n",
        "#         W = image_size[1]\n",
        "#         C = image_size[2]\n",
        "\n",
        "#         conv1 = {\n",
        "#             \"in\": 1,\n",
        "#             \"out\": 16,\n",
        "#             \"kernel\": 5,\n",
        "#             \"stride\": 2,\n",
        "#             \"padding\": 0\n",
        "#         }\n",
        "#         conv2 = {\n",
        "#             \"in\":conv1[\"out\"],\n",
        "#             \"out\": 28,\n",
        "#             \"kernel\": 5,\n",
        "#             \"stride\": 2,\n",
        "#             \"padding\": 0\n",
        "#         }\n",
        "#         pool = {\n",
        "#             \"kernel\": 2,\n",
        "#             \"stride\": 2,\n",
        "#             \"padding\": 0\n",
        "#         }\n",
        "\n",
        "#         conv_param = [conv1,pool,conv2,pool]\n",
        "\n",
        "#         for i in range (0, len(conv_param)):\n",
        "#             H = np.floor((H + 2*conv_param[i][\"padding\"] - conv_param[i][\"kernel\"])/conv_param[i][\"stride\"] + 1)\n",
        "#             W = np.floor((W + 2*conv_param[i][\"padding\"] - conv_param[i][\"kernel\"])/conv_param[i][\"stride\"] + 1)\n",
        "\n",
        "\n",
        "#         activation_fn = nn.ReLU(0.1)\n",
        "#         # Defining a Sequential pipeline for the entire CNN\n",
        "#         self.net = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels = conv_param[0][\"in\"],\n",
        "#                      out_channels = conv_param[0][\"out\"],\n",
        "#                      kernel_size = conv_param[0][\"kernel\"],\n",
        "#                      stride= conv_param[0][\"stride\"],\n",
        "#                      padding= conv_param[0][\"padding\"]),\n",
        "#             activation_fn,\n",
        "#             LocalResponseNorm(2),   # Check: https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac\n",
        "#             nn.MaxPool2d(kernel_size = conv_param[1][\"kernel\"],\n",
        "#                         stride =conv_param[1][\"stride\"]),\n",
        "            \n",
        "#             nn.Conv2d(in_channels = conv_param[2][\"in\"],\n",
        "#                      out_channels = conv_param[2][\"out\"],\n",
        "#                      kernel_size = conv_param[2][\"kernel\"],\n",
        "#                      stride= conv_param[2][\"stride\"],\n",
        "#                      padding= conv_param[2][\"padding\"]),\n",
        "#             nn.MaxPool2d(kernel_size = conv_param[3][\"kernel\"],\n",
        "#                         stride =conv_param[3][\"stride\"]),\n",
        "            \n",
        "#             #nn.Flatten(), # Flattens a contiguous range of dims into a tensor.\n",
        "#             # FFNN\n",
        "#             activation_fn,\n",
        "#             nn.Linear(int(conv_param[-2][\"out\"] * H * W), out_features),\n",
        "#             # nn.Dropout(0.5),\n",
        "#             # nn.Sigmoid(),\n",
        "#             # nn.Linear(,self.out_classes),\n",
        "#             # nn.Dropout(0.25),\n",
        "#             # nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#       return nn.Softmax(self.net(x))\n",
        "\n",
        "# NN = Model(out_features, image_shape)\n",
        "# device = device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')  # use cuda if possible\n",
        "# NN.to(device)\n",
        "# print(NN)\n",
        "\n",
        "# # Definition of optimizer and loss functions\n",
        "# optimizer = optim.Adam(NN.parameters(),lr=0.008)\n",
        "# loss_fn = nn.CrossEntropyLoss() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "f59d9b4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send train=True for training transforms and False for val/test transforms\n",
        "def get_transform(train):\n",
        "  if train:\n",
        "    return A.Compose(\n",
        "      [\n",
        "        A.HorizontalFlip(0.5),\n",
        "        # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "        ToTensorV2(p=1.0) \n",
        "      ],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )\n",
        "  else:\n",
        "    return A.Compose(\n",
        "      [ToTensorV2(p=1.0)],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de4f36b",
      "metadata": {},
      "source": [
        "## DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "9090d729",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = img_dataset(1080,1920,'proj_test/proj_img1/',transforms=get_transform(train=True))\n",
        "dataset_test = img_dataset(1080,1920,'proj_test/proj_img1/',transforms=get_transform(train=True))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "# train test split\n",
        "test_split = 0.2\n",
        "tsize = int(len(dataset)*test_split)\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:10])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "  dataset,\n",
        "  batch_size=1,\n",
        "  shuffle=True,\n",
        "  num_workers=0,\n",
        "  collate_fn=utils.collate_fn,\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "  dataset_test,\n",
        "  batch_size=10,\n",
        "  shuffle=False,\n",
        "  num_workers=0,\n",
        "  collate_fn=utils.collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e449442",
      "metadata": {},
      "source": [
        "PRETRAINED MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "954c4986",
      "metadata": {},
      "outputs": [],
      "source": [
        "## RESNET pre-trained model :)\n",
        "def get_object_detection_model(num_classes):\n",
        "  # load a model pre-trained pre-trained on COCO\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ad0eab",
      "metadata": {},
      "source": [
        "TRAINING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6b636299",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = 2 #CLASS ZERO FOR BACKGROUND, 1 FOR PLAYERS AND 2 FOR BALL \n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "  optimizer,\n",
        "  step_size=3,\n",
        "  gamma=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2d072d89",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c1c93609",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proj_test/proj_img1/000313.jpg\n",
            "Epoch: [0]  [ 0/10]  eta: 0:03:30  lr: 0.000560  loss: 0.6870 (0.6870)  loss_classifier: 0.5921 (0.5921)  loss_box_reg: 0.0000 (0.0000)  loss_objectness: 0.0730 (0.0730)  loss_rpn_box_reg: 0.0218 (0.0218)  time: 21.0822  data: 0.0996  max mem: 0\n",
            "proj_test/proj_img1/000162.jpg\n",
            "proj_test/proj_img1/000278.jpg\n",
            "proj_test/proj_img1/000391.jpg\n",
            "proj_test/proj_img1/000090.jpg\n",
            "proj_test/proj_img1/000181.jpg\n",
            "proj_test/proj_img1/000125.jpg\n",
            "proj_test/proj_img1/000562.jpg\n",
            "proj_test/proj_img1/000156.jpg\n",
            "proj_test/proj_img1/000587.jpg\n"
          ]
        }
      ],
      "source": [
        "# training for 5 epochs\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # training for one epoch\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909224bc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ismoSfa33bBb",
      "metadata": {
        "id": "ismoSfa33bBb"
      },
      "outputs": [],
      "source": [
        "# Define training function\n",
        "# HOLA \n",
        "# adios\n",
        "\n",
        "def accuracy(target, pred):\n",
        "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
        "\n",
        "# Set number of epochs according to computational power, time and results.\n",
        "batch_size = 64\n",
        "num_epochs = 4\n",
        "validation_every_steps = 500\n",
        "\n",
        "step = 0\n",
        "NN.train()\n",
        "\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "running_loss = 0\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_accuracies_batches = []\n",
        "    train_loss_batches = []\n",
        "    \n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        # Forward pass, compute gradients, perform one training step.\n",
        "        # Your code here!\n",
        "        output = NN(inputs) # Pass the inouts through the NN\n",
        "        loss = loss_fn(output,targets) # Compute and Save loss\n",
        "        optimizer.zero_grad() #Clean up gradients\n",
        "        loss.backward() # Compute gradients based on the loss from the current batch\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Increment step counter\n",
        "        step += 1\n",
        "        \n",
        "        # Compute accuracy.\n",
        "        predictions = output.max(1)[1]\n",
        "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
        "        \n",
        "        if step % validation_every_steps == 0:\n",
        "            \n",
        "            validation_loss = 0\n",
        "            # Append average training accuracy to list.\n",
        "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
        "            \n",
        "            train_accuracies_batches = []\n",
        "        \n",
        "            # Compute accuracies on validation set.\n",
        "            valid_accuracies_batches = []\n",
        "            with torch.no_grad():\n",
        "                NN.eval()\n",
        "                for inputs, targets in test_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    output = NN(inputs)\n",
        "                    loss = loss_fn(output, targets)\n",
        "\n",
        "                    predictions = output.max(1)[1]\n",
        "\n",
        "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
        "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
        "\n",
        "                NN.train()\n",
        "                \n",
        "            # Append average validation accuracy to list.\n",
        "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
        "     \n",
        "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
        "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
        "\n",
        "# Save model (i.e. its weights)\n",
        "torch.save(NN.state_dict(), 'trained_models/CNN_weights.pth')\n",
        "print(\"Finished training.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
