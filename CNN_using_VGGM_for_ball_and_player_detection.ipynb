{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivans14/Deep_Learning_Proj/blob/main/CNN_using_VGGM_for_ball_and_player_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e59f02",
      "metadata": {
        "id": "50e59f02"
      },
      "source": [
        "# Ball and players detection for SoccerNet\n",
        "### Using CNN and the pretained model VGGM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d98dbff6",
      "metadata": {
        "id": "d98dbff6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('TkAgg')\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import LocalResponseNorm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import os\n",
        "import cv2\n",
        "from files.engine import train_one_epoch, evaluate, hi\n",
        "\n",
        "import files.utils as utils\n",
        "import files.transforms as T\n",
        "from tkinter import *\n",
        "# for image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "abf99ee7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "IPN27JHX3Ra5",
      "metadata": {
        "id": "IPN27JHX3Ra5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import dataset as: train, validation and test splits\n",
        "\n",
        "data = pd.read_csv('proj_test/proj_det/det.txt')\n",
        "images= ['proj_test/proj_img1/{}.jpg'.format(str(i).zfill(6)) for i in range(750)]\n",
        "images[1]\n",
        "int(images[1][-10:-4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "68c45f52",
      "metadata": {
        "id": "68c45f52"
      },
      "outputs": [],
      "source": [
        "# Create image dataset class\n",
        "class img_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, height, width, dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.dir = dir\n",
        "        self.images=images\n",
        "\n",
        "    def __getitem__(self,id):\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        img_name = self.images[id]\n",
        "        # print(img_name)\n",
        "\n",
        "        # reading the images and converting them to correct size and color    \n",
        "        img = cv2.imread(img_name)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "        # diving by 255\n",
        "        img_res /= 255.0\n",
        "        with open ('proj_test/proj_det/det.txt') as f:\n",
        "            next(f)\n",
        "            for line in f:\n",
        "                line.strip(\"/n\")\n",
        "                parsed = [float(x) for x in line.split(',')]\n",
        "                if int(parsed[0]) == int(self.images[id][-10:-4]):\n",
        "                    x = parsed[2]\n",
        "                    y = parsed[3]\n",
        "                    width = parsed[4]\n",
        "                    height = parsed[5]\n",
        "                    x_max = x + width\n",
        "                    y_max = y + height\n",
        "                    boxes.append([x,y,x_max,y_max])\n",
        "                    labels.append(int(parsed[-1]))\n",
        "        \n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        image_id = torch.tensor([id])\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(image = img_res,\n",
        "                                        bboxes = target['boxes'],\n",
        "                                        labels = labels)\n",
        "            img_res = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "            return img_res, target\n",
        "\n",
        "        # img_res = torch.Tensor(img_res)\n",
        "\n",
        "        return img_res,boxes\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "45b0601e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1080, 1920, 3)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img_ds = img_dataset(1080,1920,'proj_test/proj_img1/')\n",
        "ran_img, boxes = img_ds[20]\n",
        "ran_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef3eefd3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mirun\\AppData\\Local\\Temp\\ipykernel_5340\\3181187.py:28: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "#Visualize the data\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def plot_img_bbox(img, boxes):\n",
        "  # plot the image and bboxes\n",
        "  # Bounding boxes are defined as follows: x-min y-min width height\n",
        "  fig, a = plt.subplots(1,1)\n",
        "  fig.set_size_inches(5,5)\n",
        "  # img_plot = mpimg.imread(img)\n",
        "  a.imshow(img)\n",
        "  count = 0\n",
        "  for box in boxes:\n",
        "\n",
        "    x, y, width, height  = box[0], box[1], box[2] - box[0], box[3] - box[1]\n",
        "    rect = patches.Rectangle(\n",
        "      (x, y),\n",
        "      width, height,\n",
        "      linewidth = 2,\n",
        "      edgecolor = 'r',\n",
        "      facecolor = 'none'\n",
        "    )\n",
        "    # Draw the bounding box on top of the image\n",
        "    a.add_patch(rect)\n",
        "    a.annotate(str(count), xy=(x,y))\n",
        "    count += 1\n",
        "  plt.show()\n",
        "img_ds = img_dataset(1080,1920,'proj_test/proj_img1/')\n",
        "ran_img, boxes = img_ds[10]\n",
        "# print(boxes)\n",
        "# img = mpimg.imread(ran_img)\n",
        "# imgplot = plt.imshow(img)\n",
        "# plt.show()\n",
        "\n",
        "plot_img_bbox(ran_img, boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "48c01c0f",
      "metadata": {
        "id": "48c01c0f"
      },
      "outputs": [],
      "source": [
        "# # CNN model\n",
        "# out_features = 2 # Ball and players\n",
        "# image_shape = (400, 400, 3)\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#     def __init__(self,out_classes, image_size):\n",
        "#         super().__init__()\n",
        "#         self.out_classes = out_classes\n",
        "#         H = image_size[0]\n",
        "#         W = image_size[1]\n",
        "#         C = image_size[2]\n",
        "\n",
        "#         conv1 = {\n",
        "#             \"in\": 1,\n",
        "#             \"out\": 16,\n",
        "#             \"kernel\": 5,\n",
        "#             \"stride\": 2,\n",
        "#             \"padding\": 0\n",
        "#         }\n",
        "#         conv2 = {\n",
        "#             \"in\":conv1[\"out\"],\n",
        "#             \"out\": 28,\n",
        "#             \"kernel\": 5,\n",
        "#             \"stride\": 2,\n",
        "#             \"padding\": 0\n",
        "#         }\n",
        "#         pool = {\n",
        "#             \"kernel\": 2,\n",
        "#             \"stride\": 2,\n",
        "#             \"padding\": 0\n",
        "#         }\n",
        "\n",
        "#         conv_param = [conv1,pool,conv2,pool]\n",
        "\n",
        "#         for i in range (0, len(conv_param)):\n",
        "#             H = np.floor((H + 2*conv_param[i][\"padding\"] - conv_param[i][\"kernel\"])/conv_param[i][\"stride\"] + 1)\n",
        "#             W = np.floor((W + 2*conv_param[i][\"padding\"] - conv_param[i][\"kernel\"])/conv_param[i][\"stride\"] + 1)\n",
        "\n",
        "\n",
        "#         activation_fn = nn.ReLU(0.1)\n",
        "#         # Defining a Sequential pipeline for the entire CNN\n",
        "#         self.net = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels = conv_param[0][\"in\"],\n",
        "#                      out_channels = conv_param[0][\"out\"],\n",
        "#                      kernel_size = conv_param[0][\"kernel\"],\n",
        "#                      stride= conv_param[0][\"stride\"],\n",
        "#                      padding= conv_param[0][\"padding\"]),\n",
        "#             activation_fn,\n",
        "#             LocalResponseNorm(2),   # Check: https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac\n",
        "#             nn.MaxPool2d(kernel_size = conv_param[1][\"kernel\"],\n",
        "#                         stride =conv_param[1][\"stride\"]),\n",
        "            \n",
        "#             nn.Conv2d(in_channels = conv_param[2][\"in\"],\n",
        "#                      out_channels = conv_param[2][\"out\"],\n",
        "#                      kernel_size = conv_param[2][\"kernel\"],\n",
        "#                      stride= conv_param[2][\"stride\"],\n",
        "#                      padding= conv_param[2][\"padding\"]),\n",
        "#             nn.MaxPool2d(kernel_size = conv_param[3][\"kernel\"],\n",
        "#                         stride =conv_param[3][\"stride\"]),\n",
        "            \n",
        "#             #nn.Flatten(), # Flattens a contiguous range of dims into a tensor.\n",
        "#             # FFNN\n",
        "#             activation_fn,\n",
        "#             nn.Linear(int(conv_param[-2][\"out\"] * H * W), out_features),\n",
        "#             # nn.Dropout(0.5),\n",
        "#             # nn.Sigmoid(),\n",
        "#             # nn.Linear(,self.out_classes),\n",
        "#             # nn.Dropout(0.25),\n",
        "#             # nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#       return nn.Softmax(self.net(x))\n",
        "\n",
        "# NN = Model(out_features, image_shape)\n",
        "# device = device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')  # use cuda if possible\n",
        "# NN.to(device)\n",
        "# print(NN)\n",
        "\n",
        "# # Definition of optimizer and loss functions\n",
        "# optimizer = optim.Adam(NN.parameters(),lr=0.008)\n",
        "# loss_fn = nn.CrossEntropyLoss() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f59d9b4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send train=True for training transforms and False for val/test transforms\n",
        "def get_transform(train):\n",
        "  if train:\n",
        "    return A.Compose(\n",
        "      [\n",
        "        A.HorizontalFlip(0.5),\n",
        "        # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "        ToTensorV2(p=1.0) \n",
        "      ],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )\n",
        "  else:\n",
        "    return A.Compose(\n",
        "      [ToTensorV2(p=1.0)],\n",
        "      bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de4f36b",
      "metadata": {},
      "source": [
        "## DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9090d729",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = img_dataset(1080,1920,'proj_test/proj_img1/',transforms=get_transform(train=True))\n",
        "dataset_test = img_dataset(1080,1920,'proj_test/proj_img1/',transforms=get_transform(train=True))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "# train test split\n",
        "test_split = 0.2\n",
        "tsize = int(len(dataset)*test_split)\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "  dataset,\n",
        "  batch_size=50,\n",
        "  shuffle=True,\n",
        "  num_workers=0,\n",
        "  collate_fn=utils.collate_fn,\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "  dataset_test,\n",
        "  batch_size=10,\n",
        "  shuffle=False,\n",
        "  num_workers=0,\n",
        "  collate_fn=utils.collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e449442",
      "metadata": {},
      "source": [
        "PRETRAINED MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "954c4986",
      "metadata": {},
      "outputs": [],
      "source": [
        "## RESNET pre-trained model :)\n",
        "def get_object_detection_model(num_classes):\n",
        "  # load a model pre-trained pre-trained on COCO\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  # replace the pre-trained head with a new one\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ad0eab",
      "metadata": {},
      "source": [
        "TRAINING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6b636299",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = 2 #CLASS ZERO FOR BACKGROUND, 1 FOR PLAYERS AND 2 FOR BALL \n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "  optimizer,\n",
        "  step_size=3,\n",
        "  gamma=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c1c93609",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 825753600 bytes.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [12], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[39m# training for one epoch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[39m# update the learning rate\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\mirun\\OneDrive\\Desktop\\ivan\\Deep_Learning_Proj\\files\\engine.py:34\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[0;32m     31\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     32\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m---> 34\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[0;32m     36\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m     38\u001b[0m \u001b[39m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:95\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     89\u001b[0m             degen_bb: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m boxes[bb_idx]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     90\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     91\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAll bounding boxes should have positive height and width.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Found invalid box \u001b[39m\u001b[39m{\u001b[39;00mdegen_bb\u001b[39m}\u001b[39;00m\u001b[39m for target at index \u001b[39m\u001b[39m{\u001b[39;00mtarget_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m             )\n\u001b[1;32m---> 95\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensors)\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     97\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:54\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[0;32m     53\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbody(x)\n\u001b[1;32m---> 54\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfpn(x)\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\ops\\feature_pyramid_network.py:149\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    146\u001b[0m results\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_result_from_layer_blocks(last_inner, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m    148\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x) \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> 149\u001b[0m     inner_lateral \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_result_from_inner_blocks(x[idx], idx)\n\u001b[0;32m    150\u001b[0m     feat_shape \u001b[39m=\u001b[39m inner_lateral\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[0;32m    151\u001b[0m     inner_top_down \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(last_inner, size\u001b[39m=\u001b[39mfeat_shape, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\ops\\feature_pyramid_network.py:112\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.get_result_from_inner_blocks\u001b[1;34m(self, x, idx)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mfor\u001b[39;00m i, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_blocks):\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m idx:\n\u001b[1;32m--> 112\u001b[0m         out \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 825753600 bytes."
          ]
        }
      ],
      "source": [
        "# training for 5 epochs\n",
        "num_epochs = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # training for one epoch\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909224bc",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ismoSfa33bBb",
      "metadata": {
        "id": "ismoSfa33bBb"
      },
      "outputs": [],
      "source": [
        "# Define training function\n",
        "# HOLA \n",
        "# adios\n",
        "\n",
        "def accuracy(target, pred):\n",
        "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
        "\n",
        "# Set number of epochs according to computational power, time and results.\n",
        "batch_size = 64\n",
        "num_epochs = 4\n",
        "validation_every_steps = 500\n",
        "\n",
        "step = 0\n",
        "NN.train()\n",
        "\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "running_loss = 0\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_accuracies_batches = []\n",
        "    train_loss_batches = []\n",
        "    \n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        # Forward pass, compute gradients, perform one training step.\n",
        "        # Your code here!\n",
        "        output = NN(inputs) # Pass the inouts through the NN\n",
        "        loss = loss_fn(output,targets) # Compute and Save loss\n",
        "        optimizer.zero_grad() #Clean up gradients\n",
        "        loss.backward() # Compute gradients based on the loss from the current batch\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Increment step counter\n",
        "        step += 1\n",
        "        \n",
        "        # Compute accuracy.\n",
        "        predictions = output.max(1)[1]\n",
        "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
        "        \n",
        "        if step % validation_every_steps == 0:\n",
        "            \n",
        "            validation_loss = 0\n",
        "            # Append average training accuracy to list.\n",
        "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
        "            \n",
        "            train_accuracies_batches = []\n",
        "        \n",
        "            # Compute accuracies on validation set.\n",
        "            valid_accuracies_batches = []\n",
        "            with torch.no_grad():\n",
        "                NN.eval()\n",
        "                for inputs, targets in test_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    output = NN(inputs)\n",
        "                    loss = loss_fn(output, targets)\n",
        "\n",
        "                    predictions = output.max(1)[1]\n",
        "\n",
        "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
        "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
        "\n",
        "                NN.train()\n",
        "                \n",
        "            # Append average validation accuracy to list.\n",
        "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
        "     \n",
        "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
        "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
        "\n",
        "# Save model (i.e. its weights)\n",
        "torch.save(NN.state_dict(), 'trained_models/CNN_weights.pth')\n",
        "print(\"Finished training.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
